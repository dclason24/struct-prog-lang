1/14/26

expression -> tokenizer -> parser -> executor -> output

                             +
                             /\
1+2*34     -> |1|2|*|24| -> 1  *   -> output
                              /\
                            2  34

tokenizer: creates a token stream and recognizes all the tokens that were written. 
separates each token
parser: creates an abstract syntax tree from the token stream representing the codes structure 
and turns it into a json. 
executor: produces the output

regular expression: way of telling if your string matches a kind of pattern
___________________________________________________________________________________________________
1/21/26

import re

patterns = [
  //r means regular expression - just signifies we are making a r expression
  (r"\s+", "whitespace"),
  (r "\d+", "number"),
  (r "\+", "+"),
  (r"\-", "-"),
  (r "\/", "/"),
  (r "\*", "*"),
  (r ".", "error")
]


patterns = [(re.compile(p), tag) for p, tag in patterns]

def tokenize(characters):
  "tokenize a string using the patterns above"
  tokens = []
  position = 0
  line = 1
  column = 1
  current_tag = None

  while position < len(characters):
    for pattern, tag in patterns:
      match = pattern.match(characters, position)
      if match:
        break
    assert match it not None
    //what are the characters that match this pattern?
    value = match.group(0)

    //look at the tag that was left behind
    if current_tag == "error"
      raise Exception(f"Unexpected character: {value!r}")
    
    if tag != "whitespace":
      token = {"tag": current_tag, "line": line, "column": column}
      if current_tag = "number":
        token["value] = int(value)
      tokens.append(token)

       #advance position and update line/column
        for ch in value:
            if ch == "\n":
                line += 1
                column = 1
            else:
                column += 1
        position = match.end()

tokens.append({"tag": None, "line": line, "column" : column})
return tokens

def test_digits():
    print("test tokenize digits")
    t = tokenize("123")
    assert t[0]["tag"] == "number"
    assert t[0]["value"] == 123
    assert t[1]["tag"] is None

def test_operators():
  t = tokenize("+-*/")
  //tokenize all but the last one
  tags = [tok["tag] for tok in t[:-1]]


if __name__ == "__main__":
    test_digits()
    print("done.")

#


